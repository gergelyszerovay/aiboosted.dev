{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fa1c6e1-e571-4a39-9500-2fc9fd586e2a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "// In Deno, we have to prefix the imports from `node_modules` with `npm:`:\n",
    "import { Ollama } from \"npm:@langchain/community/llms/ollama\";\n",
    "import { BufferMemory } from \"npm:langchain/memory\";\n",
    "import { LLMChain } from \"npm:langchain/chains\";\n",
    "import {\n",
    "  PromptTemplate,\n",
    "  ChatPromptTemplate,\n",
    "  MessagesPlaceholder,\n",
    "} from \"npm:@langchain/core/prompts\";\n",
    "import { StructuredOutputParser } from \"npm:langchain/output_parsers\";\n",
    "\n",
    "import * as diff from 'npm:diff';\n",
    "import { z } from \"npm:zod\";\n",
    "\n",
    "import { html } from \"https://deno.land/x/display/mod.ts\";"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d258242-9a1d-47da-95bd-74d30c969a06",
   "metadata": {},
   "source": [
    "#### First step: we send our text to the LLM for review, and we get the model's response as a Javascript object containing the reviewed text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "44c6e5e5-569b-4713-a1e2-3bc9db8952fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Basic LangChain chains contain a prompt, a model and an output parser. Our chain:\n",
    "// 1. creates a prompt from the user inputs: `instruction` and `inputText`\n",
    "// 2. sends it to an LLM, the LLM responds with a JSON string\n",
    "// 3. parses the LLM's JOSN output to a Javscript object\n",
    "\n",
    "// 1. This is our prompt, it has two inputs: `instruction` and `inputText`. We'll assign values for these inputs, when we execute the chain\n",
    "const prompt = PromptTemplate.fromTemplate(`\n",
    "{instruction}\n",
    "{inputText}\n",
    "`);\n",
    "\n",
    "// 2. We create an Ollama LangChain LLM model that uses the local codellama:7b-code model. This is a small model, requires about 4 GB of RAM. \n",
    "// I use this model on my older laptop that has only 16 GB RAM and a dual core CPU, and it gives responses with an acceptable speed. \n",
    "// Bigger models, like llama2:70b give better results, but they require a computer with 64 GB of RAM and a fast CPU/GPU.\n",
    "const llm = new Ollama({\n",
    "  baseUrl: \"http://localhost:11434\",\n",
    "  model: \"codellama:7b-code\",\n",
    "  verbose: false,\n",
    "});\n",
    "\n",
    "// 3. Output parser: We define a Zod schema. LangChain will parse the LLM model's output using this schema, and return a Javascript object\n",
    "//    at the end of the chain in the following data structure: `{ \"reviewedText\": \"the reviewed text\" }`\n",
    "const ResponseZod = z.object({ reviewedText: z.string() });\n",
    "type ResponseType = z.infer<typeof ResponseZod>;\n",
    "const outputParser = StructuredOutputParser.fromZodSchema(ResponseZod);\n",
    "\n",
    "// the chain contains the prompt, the model and the output parser\n",
    "const chain = prompt.pipe(llm).pipe(outputParser);\n",
    "\n",
    "// we specify the inputs for the prompt\n",
    "const instruction = 'Fix the grammar issues in the following text. Return the reviewed text in JSON only, using the following format: ' +\n",
    "    '{ \"reviewedText\": \"the reviewed text\" }';\n",
    "const inputText = \"How to stays relevant as the developer in the world of ai?\";\n",
    "\n",
    "// finally we execute the chain and get the response as a JavaScript object\n",
    "const response: ResponseType = await chain.invoke({ instruction, inputText });"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d58413-ff6d-4742-b6cd-ba746f59e331",
   "metadata": {},
   "source": [
    "#### Second step: we compare our original text and the LLM's response, and show the differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4fa88024-7569-46f5-914d-e3c794ce944b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"background: black; color: white; padding: 10px;\"><span>How to stay</span><span style=\"color: red; text-decoration: line-through;\">s</span><span> relevant as </span><span style=\"color: red; text-decoration: line-through;\">the</span><span style=\"color: lightgreen;\">a</span><span> developer in the world of </span><span style=\"color: red; text-decoration: line-through;\">ai</span><span style=\"color: lightgreen;\">AI</span><span>?</span></div>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// We compare the original text and the LLM's response\n",
    "const diffChanges = diff.diffChars(inputText, response.reviewedText);\n",
    "\n",
    "// We show the differences between our original text and the LLM's response in HTML\n",
    "const h = diffChanges.map((r) => {\n",
    "  if (!r.added && !r.removed) { return `<span>${r.value}</span>`; } \n",
    "  if (r.added) { return `<span style=\"color: lightgreen;\">${r.value}</span>`; } \n",
    "  if (r.removed) { return `<span style=\"color: red; text-decoration: line-through;\">${r.value}</span>`; } \n",
    "}).join(\"\");\n",
    "    \n",
    "html`<div style=\"background: black; color: white; padding: 10px;\">${h}</div>`;"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deno",
   "language": "typescript",
   "name": "deno"
  },
  "language_info": {
   "file_extension": ".ts",
   "mimetype": "text/x.typescript",
   "name": "typescript",
   "nb_converter": "script",
   "pygments_lexer": "typescript",
   "version": "5.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
