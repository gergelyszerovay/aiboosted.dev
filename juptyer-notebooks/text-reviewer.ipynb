{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f567bfee-25e4-4aea-8897-2098d7d8cbc6",
   "metadata": {},
   "source": [
    "### Imports\n",
    "\n",
    "In Deno, we have to prefix the imports from npmjs.org with `npm:`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18b913fb-025c-4e5f-bdab-4532d16ed094",
   "metadata": {},
   "outputs": [],
   "source": [
    "import { BufferMemory } from \"npm:langchain/memory\";\n",
    "import { LLMChain } from \"npm:langchain/chains\";\n",
    "import {\n",
    "  PromptTemplate,\n",
    "  ChatPromptTemplate,\n",
    "  MessagesPlaceholder,\n",
    "} from \"npm:@langchain/core/prompts\";\n",
    "import { StructuredOutputParser } from \"npm:langchain/output_parsers\";\n",
    "\n",
    "import * as diff from 'npm:diff';\n",
    "import { z } from \"npm:zod\";\n",
    "\n",
    "import { html } from \"https://deno.land/x/display/mod.ts\";"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a62e42-2b6f-4e55-a033-35ff2366cbb2",
   "metadata": {},
   "source": [
    "## First step: we send our text to the LLM for review, and we get the model's response as a Javascript object containing the reviewed text\n",
    "\n",
    "### Basic LangChain chains contain a prompt, a model and an output parser\n",
    "\n",
    "#### Prompt\n",
    "\n",
    "This is our prompt, it has two inputs: `instruction` and `inputText`. We'll assign values for these inputs, when we execute the chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00227f0a-ea17-468d-8235-d50104535453",
   "metadata": {},
   "outputs": [],
   "source": [
    "const prompt = PromptTemplate.fromTemplate(`\n",
    "{instruction}\n",
    "{inputText}\n",
    "`);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc572324-a8bd-4a2f-a676-dcca6a791f96",
   "metadata": {},
   "source": [
    "### Chat model: codellama\n",
    "\n",
    "We create an Ollama LLM model that uses the local [codellama:7b-code](https://ollama.com/library/codellama:7b-code) model. This is a small model, requires about 4 GB of RAM. I use this model on my older laptop that has only 16 GB RAM and a dual core CPU, and it gives responses with an acceptable speed. Bigger models, like [llama2:70b](https://ollama.com/library/llama2:70b) give better results, but they require a computer with 64 GB of RAM and a fast CPU/GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4fd3349e-aef7-4b67-b783-0efc933b5876",
   "metadata": {},
   "outputs": [],
   "source": [
    "import { Ollama } from \"npm:@langchain/community/llms/ollama\";\n",
    "const llm = new Ollama({\n",
    "  baseUrl: \"http://localhost:11434\",\n",
    "  model: \"codellama:7b-code\",\n",
    "  verbose: true,\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7083ae88-f28a-44b8-adf3-b91f25026b11",
   "metadata": {},
   "source": [
    "### Chat model: OpenAI\n",
    "\n",
    "With LangChain, we can use the same API with a wide variety of LLM models, so for example we can replace the `new ChatOllama(...)` part of the code with the following to use OpenAI's LLM model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec7a17f8-379e-42c0-ab62-c5026a9c379e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "// import { OpenAI } from \"npm:@langchain/openai\";\n",
    "\n",
    "// // auto-load .env\n",
    "// import \"https://deno.land/std@0.215.0/dotenv/load.ts\";\n",
    "\n",
    "// const llm = new OpenAI({ \n",
    "//   modelName: \"gpt-3.5-turbo\", \n",
    "//   verbose: true,\n",
    "// });"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fed101-dbcd-472a-bbc1-4a36439cb7fe",
   "metadata": {},
   "source": [
    "### Output parser\n",
    "\n",
    "We define a Zod schema. LangChain will parse the LLM model's output using this schema, and return a Javascript object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92567091-cfee-443d-a2e4-975e1df3104e",
   "metadata": {},
   "outputs": [],
   "source": [
    "const outputParser = StructuredOutputParser.fromZodSchema(\n",
    "  z.object({\n",
    "    reviewedText: z.string()\n",
    "  })\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39e3f2b-4bdb-432d-bdab-eb5604395425",
   "metadata": {},
   "source": [
    "### We assembly the chain and execute it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5dd4f4ae-0aee-4f90-be40-2611e0a216b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[llm/start]\u001b[39m [\u001b[90m\u001b[1m1:llm:Ollama\u001b[22m\u001b[39m] Entering LLM run with input: {\n",
      "  \"prompts\": [\n",
      "    \"Fix the grammar issues in the following text. Return the reviewed text in JSON only, using the following format: { \\\"reviewedText\\\": \\\"the reviewed text\\\" }\\nHow to stays relevant as the developer in the world of ai?\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36m[llm/end]\u001b[39m [\u001b[90m\u001b[1m1:llm:Ollama\u001b[22m\u001b[39m] [36.19s] Exiting LLM run with output: {\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{\\n\\\"reviewedText\\\":\\\"How to stay relevant as a developer in the world of AI?\\\"\\n}\"\n",
      "      }\n",
      "    ]\n",
      "  ]\n",
      "}\n",
      "{\n",
      "  reviewedText: \u001b[32m\"How to stay relevant as a developer in the world of AI?\"\u001b[39m\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "// the chain contains the prompt, the model and the output parser\n",
    "const chain = prompt.pipe(llm).pipe(outputParser);\n",
    "\n",
    "// we specify the inputs of the prompt\n",
    "const instruction = \"Fix the grammar issues in the following text. Return the reviewed text in JSON only, using the following format: \" +\n",
    "    `{ \"reviewedText\": \"the reviewed text\" }`;\n",
    "const inputText = \"How to stays relevant as the developer in the world of ai?\";\n",
    "\n",
    "// finaly we execute the chain\n",
    "const response = await chain.invoke({ instruction, inputText });\n",
    "\n",
    "// and show the response from the LLM\n",
    "console.log(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4085117a-2ab0-4166-be3d-16f80097f836",
   "metadata": {},
   "source": [
    "## Second step: we compare our original text and the LLM's response, and show the differences\n",
    "#### We compare our original text and the LLM's response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a298c4e7-8c27-423a-b229-ff00c8d2e21e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  { count: \u001b[33m11\u001b[39m, value: \u001b[32m\"How to stay\"\u001b[39m },\n",
      "  { count: \u001b[33m1\u001b[39m, added: \u001b[90mundefined\u001b[39m, removed: \u001b[33mtrue\u001b[39m, value: \u001b[32m\"s\"\u001b[39m },\n",
      "  { count: \u001b[33m13\u001b[39m, value: \u001b[32m\" relevant as \"\u001b[39m },\n",
      "  { count: \u001b[33m3\u001b[39m, added: \u001b[90mundefined\u001b[39m, removed: \u001b[33mtrue\u001b[39m, value: \u001b[32m\"the\"\u001b[39m },\n",
      "  { count: \u001b[33m1\u001b[39m, added: \u001b[33mtrue\u001b[39m, removed: \u001b[90mundefined\u001b[39m, value: \u001b[32m\"a\"\u001b[39m },\n",
      "  { count: \u001b[33m27\u001b[39m, value: \u001b[32m\" developer in the world of \"\u001b[39m },\n",
      "  { count: \u001b[33m2\u001b[39m, added: \u001b[90mundefined\u001b[39m, removed: \u001b[33mtrue\u001b[39m, value: \u001b[32m\"ai\"\u001b[39m },\n",
      "  { count: \u001b[33m2\u001b[39m, added: \u001b[33mtrue\u001b[39m, removed: \u001b[90mundefined\u001b[39m, value: \u001b[32m\"AI\"\u001b[39m },\n",
      "  { count: \u001b[33m1\u001b[39m, value: \u001b[32m\"?\"\u001b[39m }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "const diffChanges = diff.diffChars(inputText, response.reviewedText);\n",
    "console.log(diffChanges)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b80f27-9f49-4c17-8964-fe8803c9588b",
   "metadata": {},
   "source": [
    "#### We show the differences between our original text and the LLM's response in HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "23a0c2cb-30b8-43ab-9a6f-24fc2da9ef05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"background: black; color: white; padding: 10px;\"><span>How to stay</span><span style=\"color: red; text-decoration: line-through;\">s</span><span> relevant as </span><span style=\"color: red; text-decoration: line-through;\">the</span><span style=\"color: lightgreen;\">a</span><span> developer in the world of </span><span style=\"color: red; text-decoration: line-through;\">ai</span><span style=\"color: lightgreen;\">AI</span><span>?</span></div>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "const h = diffChanges.map((r) => {\n",
    "  if (!r.added && !r.removed) {\n",
    "    return `<span>${r.value}</span>`;\n",
    "  } \n",
    "  if (r.added) {\n",
    "    return `<span style=\"color: lightgreen;\">${r.value}</span>`;\n",
    "  } \n",
    "  if (r.removed) {\n",
    "    return `<span style=\"color: red; text-decoration: line-through;\">${r.value}</span>`;\n",
    "  } \n",
    "}).join(\"\");\n",
    "    \n",
    "html`<div style=\"background: black; color: white; padding: 10px;\">${h}</div>`;"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deno",
   "language": "typescript",
   "name": "deno"
  },
  "language_info": {
   "file_extension": ".ts",
   "mimetype": "text/x.typescript",
   "name": "typescript",
   "nb_converter": "script",
   "pygments_lexer": "typescript",
   "version": "5.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
